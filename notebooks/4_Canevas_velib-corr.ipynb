{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canevas pour le TP Vélib\n",
    "\n",
    "**Travailler sur des données réelles**\n",
    "\n",
    "On n'a pas le temps de travailler sur des API réelles, on va sauter l'étape de récupération des données sur internet et aller directement à la phase d'analyse... Mais ces données n'en sont pas moins réelles et bruitées.\n",
    "\n",
    "Ce notebook permet aussi de consolider les acquis sur `pandas`, `numpy` ainsi que sur les fonctions d'affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports nécessaires dans le cadre du TP\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# en cas de version <0.20:\n",
    "# ! pip install pandas --user --proxy=proxy.ufr-info-p6.jussieu.fr:3128 --upgrade\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des données dans pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nom de fichier (chemin d'accès à personnaliser)\n",
    "fnamejs = 'data/dataVelibJSON.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture du fichier\n",
    "f= open(fnamejs,'r') # ouverture en lecture\n",
    "data = f.read() # récupération d'une chaine de caractères... Données vraiment très brutes !\n",
    "print(data[:500]) # voir ces données brutes (les 500 premiers caractères)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passage automatique à une table contenant une entrée par ligne\n",
    "\n",
    "**input** = '[{\"status\": \"OPEN\", \"contract_name\": \"Paris\", \"name\": \"\", \"bonus\": \"True\", \"bike_stands\": 50, \"number\": 31705, \"last_update\": 1410616150000, \"available_bike_stands\": 48, \"banking\": \"True\", \"available_bikes\": 1, \"address\": \"\", \"lat\": 48.8645278209514, \"lng\": 2.416170724425901, \"alt\": 74.37134552001953}, {\"status\": \"OPEN\", ...\n",
    "\n",
    "**output**=\n",
    " $$D =  \\left[\\begin{array}{cccc}\n",
    " ind & A & B & C \\\\1 & x_{1}& y_{1} &z_{1}\\\\ & \\vdots & \\\\\\ell&  x_{\\ell}& y_{\\ell} &z_{\\ell}\n",
    " \\\\ & \\vdots & \\\\ N&  x_{N}& y_{N} &z_{N}\n",
    " \\end{array}\n",
    " \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(data, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tester et comprendre le fonctionnement des méthodes suivantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head()) # entete + 5 lignes => comprendre les différentes variables aléatoires descriptives\n",
    "#print(df.index)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_index(axis=1, ascending=False)\n",
    "# methode de selection de donnée:\n",
    "df['lng']                   # recupération d'une colonne\n",
    "df.loc[:,['lat','lng']]     # recupération de deux colonnes\n",
    "\n",
    "df[df['lat']>48.85]         # sélection des données\n",
    "df['status'].value_counts() # récupération des valeurs possibles + comptage des occurences des valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des données au format numpy\n",
    "df['alt'].to_numpy()\n",
    "df['alt'].hist() # histogramme des altitudes à Paris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> EXO Transformation des données</span>\n",
    "\n",
    "Définir une nouvelle colonne correspondant aux arrondissements parisiens\n",
    "1. Nouvelle colonne:\n",
    "```df['arr'] = ...```\n",
    "1. Analyse de la colonne `number`: les deux premiers chiffres donnent l'arrondissement\n",
    "1. Donner le comptage des valeurs (nombre de stations par arrondissement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <CORRECTION>\n",
    "df['arr'] = df['number']//1000\n",
    "df['arr'].value_counts()\n",
    "# </CORRECTION>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'étude de cette variable fait apparaitre des stations hors des arrondissements... Nous allons les supprimer\n",
    "\n",
    "Cette opération est non-triviale: il faut sélectionner les stations $\\le$ 20.... Mais aussi les stations avec un arrondissement $>0$!\n",
    "\n",
    "Pour effectuer cette opération, le plus simple est d'utiliser `np.logical_and` qui agrège efficacement les booléens contenus dans deux vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<CORRECTION>\n",
    "# pour travailler sur Paris uniquement:\n",
    "dfp = df[np.logical_and(df['arr']>0,df['arr']<=20)].copy() # pour travailler sur une nouvelle structure de données\n",
    "df = dfp # pour travailler sur df pendant tout le TP... le df d'origine est ici perdu.\n",
    "#</CORRECTION>\n",
    "\n",
    "# pour travailler sur Paris uniquement:\n",
    "# dfp =    # faire la sélection \n",
    "# df = dfp # pour travailler sur df pendant tout le TP... le df d'origine est ici perdu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fonctions d'affichage\n",
    "\n",
    "Beaucoup de fonctions numpy/matplotlib/pandas sont compatibles entre elles. Démonstration ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all') # tout fermer\n",
    "plt.figure() # nouvelle figure\n",
    "plt.scatter(df['lng'],df['lat']) # scatter plot\n",
    "plt.show() # affichage de la figure en cours\n",
    "\n",
    "# note: on peut obtenir le même genre de résultats avec plot, mais c'est moins joli !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etude de l'altitude\n",
    "\n",
    "1. Le code donne une couleur pour l'altitude des stations\n",
    "1. <span style=\"color:red\"> EXO</span> Modifier ce code (ou ouvrir une nouvelle boite) pour mettre en évidence l'arrondissement d'appartenance de la station\n",
    "    * le résultat par défaut est peu contrasté... Idéalement, il faut jouer sur le code couleur ou choisir les couleurs à la main avec une boucle\n",
    "1. <span style=\"color:red\"> EXO</span> Modifier ce code (ou ouvrir une nouvelle boite) pour mettre en évidence le taux de remplissage des stations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "coll = ax.scatter(df['lng'],df['lat'] , s=10, c=df['alt'])\n",
    "plt.grid(True) # affichage de la grille\n",
    "plt.colorbar(coll) # affichage de la légende\n",
    "\n",
    "#<CORRECTION>\n",
    "plt.figure()\n",
    "plt.scatter(df['lng'],df['lat'], s=10, c=df['arr'], cmap='Accent')\n",
    "plt.grid(True) # affichage de la grille\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(df['lng'],df['lat'], s=10, c=df['available_bike_stands']/df['bike_stands'])\n",
    "plt.grid(True) # affichage de la grille\n",
    "#</CORRECTION>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul d'aggrégat complexe\n",
    "\n",
    "La fonction crosstab de pandas permet de calculer des choses sur des facteurs croisés: on obtient ainsi des tables de contingence ou des statistiques plus avancées. \n",
    "Nous allons nous intéresser à la distribution des emplacements par arrondissement. Cet exemple est abordable car les distributions de probabilité sont dicrètes.\n",
    "\n",
    "https://pbpython.com/pandas-crosstab.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Combien de stations avec N places dans les différents arrondissement?')\n",
    "plt.figure()\n",
    "plt.imshow(pd.crosstab(df['arr'], df['bike_stands']))\n",
    "plt.show()\n",
    "\n",
    "# attention, crosstab => structure pandas de type int...\n",
    "# conversion = \n",
    "comptage = pd.crosstab(df['arr'], df['bike_stands']).to_numpy().astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices\n",
    "\n",
    "Etudier la distribution de probabilité entre altitude et disponibilité. Cet exercice est un exercice à tiroirs... Comportant les étapes suivantes:\n",
    "\n",
    "1. Discrétisation de l'altitude pour travailler plus facilement\n",
    "1. Définition de la disponibilité en pourcentage (nb de dispo / nb de stands)\n",
    "1. Discrétisation de la disponibilité\n",
    "1. Calcul de la matrice de contingence (`crosstab`)\n",
    "1. Normalisation pour estimer la distribution $P(Alt, Dispo)$\n",
    "    * Quel problème se pose?\n",
    "1. Calcul de la distribution conditionnelle $P(Dispo | Alt)$\n",
    "1. Afficher les stations dans un repère altitude / disponibilité\n",
    "    * afficher la covariance de ces deux variables aléatoires et le coefficient de corrélation: proposer une interprétation des résultats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rappels sur le calcul d'une distribution conditionnelle\n",
    "\n",
    "Pour mieux distinguer les différents cas de figure de remplissage des stations, nous décidons de séparer les différentes classes d'altitude. Ainsi, nous allons caractériser la disponibilité des Vélibs dans différents univers.\n",
    "\n",
    "* Calculer P_D_A = \n",
    "$$P(D | A) = \\frac{P(D, A)}{P(A)}$$\n",
    "    * identifer les dimensions de la matrice cible\n",
    "    * vérifier que $\\forall i,\\ \\sum_j P(D = d_j | A= a_i) = 1$ [sanity check]\n",
    "* Calculer l'espérance de disponibilité sachant l'altitude E_D_A:\n",
    "$$\\forall i,\\qquad E[D|A = a_i] = \\sum_j d_j P(D = d_j | A= a_i)$$\n",
    "Comment s'interprète cette espérance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paramétrisation de la discrétisation\n",
    "nA = 15\n",
    "nD = 12 # on ne prend pas les mêmes dimensions pour éviter de laisser trainer des bugs\n",
    "\n",
    "# discrétisation de l'altitude\n",
    "index = pd.cut(df['alt'], nA, labels=False)\n",
    "\n",
    "# visualiser la nouvelle variable pour comprendre le processus... Puis en faire une nouvelle colonne dans les données\n",
    "\n",
    "#<CORRECTION>\n",
    "\n",
    "# definition + discretisation\n",
    "#####################################\n",
    "df['pc_available'] = df['available_bikes'] / df['bike_stands']\n",
    "\n",
    "\n",
    "# passage aux index de classes & ajout de colonne dans la structure de données df\n",
    "df['ind_alt'] = pd.cut(df['alt'],nA, labels=False) # labels=False => pour avoir des entiers\n",
    "df['ind_pc']  = pd.cut(df['pc_available'],nD, labels=False) # pour avoir des entiers\n",
    "\n",
    "# </CORRECTION>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calcul des probabilites jointes (Altitude, Disponibilité)\n",
    "\n",
    "# <CORRECTION>\n",
    "p_AD = pd.crosstab(df['ind_alt'], df['ind_pc']).to_numpy().astype(float)\n",
    "print(p_AD.shape)\n",
    "p_AD /= len(df) # étape 2 : normalisation => estimation fréquentielle de la loi jointe\n",
    "\n",
    "# </CORRECTION>\n",
    "\n",
    "# affichage\n",
    "plt.figure()\n",
    "plt.imshow(p_AD)\n",
    "plt.colorbar()\n",
    "\n",
    "# plt.savefig('p_AD.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########\n",
    "# conditionnalisation\n",
    "\n",
    "# <CORRECTION>\n",
    "p_A = p_AD.sum(1) # marginalisation\n",
    "# print(len(p_A))\n",
    "\n",
    "# p(D | A) = p(A,D) / p(A)\n",
    "p_D_A = p_AD.copy() # calcul de p(D | A) : étape 1, initialisation\n",
    "# print(p_D_A.shape)\n",
    "for i in range(nA): # calcul de p(D | A) : étape 2, pour chaque ligne, division par la marginale\n",
    "    print(i,p_D_A[i])\n",
    "    p_D_A[i] /= p_A[i]\n",
    "\n",
    "# </CORRECTION>\n",
    "\n",
    "# affichage du résultat\n",
    "plt.figure()\n",
    "plt.imshow(p_D_A)\n",
    "plt.colorbar()\n",
    "\n",
    "# plt.savefig('p_D_A.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########\n",
    "# Espérances conditionnelles\n",
    "\n",
    "# <CORRECTION>\n",
    "d = (np.linspace(0, 1.0, nD+1) + 1./(2*nD))[:-1]\n",
    "\n",
    "# version rapide (mais dure à lire) reposant sur le dispatch python:\n",
    "e_D_A = (p_D_A * d.reshape(1,nD)).sum(1)\n",
    "print(e_D_A)\n",
    "\n",
    "# version élégante\n",
    "e_D_A = (p_D_A @ d.reshape(nD,1))\n",
    "print(e_D_A)\n",
    "\n",
    "\n",
    "# version plus longue (et plus lisble):\n",
    "# etape 1: valeur de disponibilité\n",
    "# entre la valeur min et la valeur max, je veux (Nd+1) jalons, \n",
    "# puis je veux la valeur centrale des intervalles (décalage 1./(2*Nd))\n",
    "# puis l'abandonne la valeur de trop (qui dépasse) = tous les indices jusqu'à l'avant dernier => [:-1]\n",
    "\n",
    "\n",
    "e_D_A = np.zeros(nA)\n",
    "# étape 2: pour chaque distribution, calcul de l'espérance:\n",
    "for i in range(nA):\n",
    "    e_D_A[i] = (d * p_D_A[i]).sum()\n",
    "print(e_D_A)\n",
    "\n",
    "\n",
    "# </CORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de correlation\n",
    "\n",
    "# <CORRECTION>\n",
    "plt.figure()\n",
    "plt.scatter(df['alt'], df['pc_available'], s=2)\n",
    "\n",
    "co = np.cov(df['alt'], df['pc_available'])[0,1] \n",
    "r  = np.cov(df['alt'], df['pc_available'])[0,1]/(np.std(df['alt']) * np.std(df['pc_available']))\n",
    "\n",
    "print('COV = {} \\nCoef de corrélation linéaire : {}'.format(co,r))\n",
    "#</CORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note: il existe des fonctions avancées dans la toolbox de stats qui font tous ces calculs automatiquement\n",
    "\n",
    "plt.figure()\n",
    "# ['ind_alt'] => indice d'altitude discrétisée ['ind_pc'] => indice de pourcentage de disponibilité\n",
    "sns.jointplot(df['ind_alt'],df['ind_pc'], kind=\"hex\", color=\"k\") \n",
    "# plt.savefig('sns_p_AD.pdf')\n",
    "\n",
    "# avec la regression\n",
    "plt.figure()\n",
    "# tracé en continu... On ne visualise pas vraiment la distribution jointe\n",
    "sns.jointplot(df['alt'],df['pc_available'], kind=\"reg\", color=\"k\")\n",
    "# plt.savefig('sns_p_AD_reg.pdf')\n",
    "\n",
    "\n",
    "# logique de filtrage sur les distributions continues\n",
    "plt.figure()\n",
    "# tracé en continu + lissage => on visualise bien la distribution jointe !\n",
    "sns.jointplot(df['alt'],df['pc_available'], kind=\"kde\", color=\"b\")\n",
    "# plt.savefig('sns_p_AD_smooth.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour aller plus loin\n",
    "\n",
    "### Indépendance\n",
    "\n",
    "L'indépendance est un phénomène critique lors de l'implémentation des méthodes... Avec deux variables aléatoires, il suffit de tester:\n",
    "$$ X \\perp Y \\iff \\forall i,j p(X = x_{i}, Y=y_{j})  = p(X = x_{i})\\times p( Y=y_{j})$$\n",
    "... Ce qui n'est jamais vérifié exactement sur des données réelles.\n",
    "\n",
    "**La bonne question est donc: suis-je assez proche d'un phénomène d'indépendance?**\n",
    "\n",
    "#### taille des stations (S) VS arrondissements (Arr)\n",
    "\n",
    "* Etude de corrélation sur la taille des stations par rapport aux arrondissements\n",
    "    * tracé de la distribution jointe (sns.jointplot)\n",
    "    * calcul du coefficient de corrélation\n",
    "\n",
    "=> la faible valeur de coefficient de corrélation de corrélation nous donne un indice, mais nous nous rappelons que dans ce sens là, ce n'est pas une démonstration\n",
    "\n",
    "* Calcul d'indépendance exact:\n",
    "    * Discrétiser (ou plutot redistribuer) les tailles de stations sur 10 valeurs\n",
    "    * Calcul de la jointe P_ArrS (cf P_AD)\n",
    "        * Attention aux indices arr entre 1 et 20 => indices entre 0 et 19\n",
    "    * Calcul des marginales P_Arr, P_S (trivial à partir de la loi jointe)\n",
    "    * Calcul de PI_ArrS = P_Arr x P_S \n",
    "        * Implémentation du calcul par double boucle => trivial\n",
    "        * calcul matriciel => non trivial (il faut dessiner les matrices sur une feuille de brouillon)\n",
    "            * transformation des vecteurs en matrice + usage de dot <br>\n",
    "        PI_ArrS = P_Arr.reshape(Narr, 1).dot(P_S.reshape(1,Ns))\n",
    "        * Comparaison des valeurs PI_ArrS vs P_ArrS <br>\n",
    "        'diff = ((PI_ArrS - P_ArrS)**2).sum()'\n",
    "        => aucune chance d'arriver à 0...\n",
    "\n",
    "\n",
    "* Application du test de $\\chi^2$ = mesure d'une distance entre distribution\n",
    "    * Lien wikipedia : [https://fr.wikipedia.org/wiki/Test_du_χ²]\n",
    "    * Lien interne : [http://mapsi.lip6.fr/pmwiki.php?n=Cours.Semaine5]\n",
    "    * Mesure de la distance entre deux distributions  $P_t$ (distribution théorique, issue des marginales dans notre exemple) et $P_o$ (distribution jointe) \n",
    "    $$D = \\sum_{i}\\sum_j N \\frac{(P_t(i,j) - P_o(i,j))^2}{P_t(i,j)}$$\n",
    "    La mesure dépend du nombre d'observation $N$\n",
    "    * Chaque distribution est caractérisée par un nombre de degrés de libertés qui vaut ici: \n",
    "    $$DoF = (|Arr| - 1)(|S|-1) = 171$$\n",
    "    * La distance limite, avec $\\alpha$ de marge d'erreur, est donnée par :<br>\n",
    "    import scipy.stats as stats<br>\n",
    "    stats.chi2.ppf($\\alpha$, DoF)\n",
    "    * **Peut-on conclure que l'arrondissement est indépendant de la taille des stations?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation de données, réduction de dimensionnalité\n",
    "\n",
    "Il serait intéressant de visualiser la population des stations en tenant compte de leur altitude, histoire de détecter les stations qui sont au bord des grandes montées.\n",
    "\n",
    "Il s'agit donc de visualiser des données 3D en 2D, donc de réduire la dimensionnalité des données.\n",
    "\n",
    "L'algorithme de l'état de l'art pour effectuer cette opération (délicate) est TSNE\n",
    "\n",
    "Les opérations à mener sont, dans l'ordre:\n",
    "1. Extraction des données au format numpy : pandas => numpy\n",
    "1. Normalisation des données (elles sont trop *serrées* par défaut et l'algorithme ne marche pas bien)\n",
    "    * **Seule chose restant à faire dans cet exercice**\n",
    "    * Colonne par colonne:\n",
    "        * Retirer la moyenne pour centrer la variable descriptive en 0\n",
    "        * Diviser par l'écart-type pour obtenir un écart-type unitaire sur la variable\n",
    "1. Invocation de TSNE pour passer de 3D à 2D\n",
    "1. Récupération des indices d'arrondissements pour faire un affichage plus joli (et surtout plus intelligible)\n",
    "1. Affichage\n",
    "\n",
    "Comment interpréter ce que vous avez sous les yeux?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.manifold as visu # accès à TSNE\n",
    "\n",
    "# pour afficher les différents arrondissements de différentes couleurs (et forme)\n",
    "style = [(s,c) for s in \"o^<*\" for c in \"byrmck\" ]\n",
    "\n",
    "# passage dans un univers numpy\n",
    "X = df.loc[:,['lat','lng','alt']].to_numpy()\n",
    "A = df['arr'].to_numpy()\n",
    "# TODO : centrage des données en [0, 0, 0]\n",
    "X = X-X.mean(0) # avec dispatch\n",
    "# TODO : ecart type unitaire sur chacune des trois variables\n",
    "X = X-X.std(0) # avec dispatch\n",
    "\n",
    "# réduction de la dimensionnalité\n",
    "X2d = visu.TSNE(perplexity=5).fit_transform(X)\n",
    "plt.figure()\n",
    "indexes = np.unique(A)\n",
    "for i in range(len(indexes)): # affichage arrondissement par arrondissement\n",
    "    ind = indexes[i]\n",
    "    plt.scatter(X2d[A==ind,0],X2d[A==ind,1] , s=10,\\\n",
    "     marker=style[int(ind)%len(style)][0],c=style[int(ind)%len(style)][1])\n",
    "plt.legend(indexes)\n",
    "# plt.savefig('stations_tsne_arr.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Tests d'indépendance\n",
    "\n",
    "# arrondissement VS taille des stations\n",
    "# Y a t il plus de vélos par station (au total, pas seulement dispo) dans le 1er ou dans le 20e arrondissement?\n",
    "\n",
    "plt.figure() # affichage de cette seconde distribution jointe => beaucoup plus ambigue\n",
    "sns.jointplot(df['arr'],df['bike_stands'], kind=\"kde\", color=\"k\")\n",
    "plt.savefig('sns_p_ArrS_smooth.pdf')\n",
    "\n",
    "co = np.cov(df['arr'], df['bike_stands'])[0,1] \n",
    "r  = np.cov(df['arr'], df['bike_stands'])[0,1]/(np.std(df['arr']) * np.std(df['bike_stands']))\n",
    "\n",
    "print('COV = {} \\nCoef de corrélation linéaire : {}'.format(co,r))\n",
    "\n",
    "# Coef de corrélation très faible... Il va falloir étudier l'indépendance des variables\n",
    "\n",
    "#discrétisation\n",
    "N = 10\n",
    "\n",
    "df['bike_stands_cat'] = pd.cut(df['bike_stands'], N, labels=False)\n",
    "\n",
    "p_ArrS = np.zeros((20,N)) # nb Arr x nb catégories de taille de station\n",
    "for i in range(len(df)): # contingence\n",
    "    #p_ArrS[df['arr'].as_matrix()[i] - 1, df['bike_stands_cat'].as_matrix()[i]] += 1\n",
    "    p_ArrS[df['arr'].to_numpy()[i] - 1, df['bike_stands_cat'].to_numpy()[i]] += 1\n",
    "p_ArrS /= len(df['arr']) # normalisation\n",
    "\n",
    "p_Arr = p_ArrS.sum(1) # marginales\n",
    "p_S = p_ArrS.sum(0)\n",
    "\n",
    "# calcul de la loi jointe théorique si les deux variables étaient independantes...\n",
    "# on aurait alors p(A,S) = p(A) * p(S) produit des marginales\n",
    "pi_ArrS = p_Arr.reshape(20, 1).dot(p_S.reshape(1,N)) \n",
    "\n",
    "# je dois ensuite calculer la distance entre la distribution jointe et la distribution théorique d'indépendance\n",
    "D = (len(df['arr']) * (p_ArrS - pi_ArrS) **2 / pi_ArrS).sum()\n",
    "# v1 = 295.74395209176612\n",
    "\n",
    "# calcul des degrés de libertés\n",
    "DoF = (len(p_S)-1) * (len(p_Arr) - 1)\n",
    "\n",
    "# limite de distance pour considérer que les distributions se ressemblent:\n",
    "limite = stats.chi2.ppf(0.05, DoF)\n",
    "# vlim = 141.760035567\n",
    "\n",
    "# avec 5% d'erreur possible, quelle est la limite pour laquelle les distributions sont définitivement trop éloignées?\n",
    "# si D > cette valeur => trop eloigné => pas indépendant\n",
    "print(D, limite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### <CORRECTION> ###\n",
    "import re\n",
    "# transformation de cet énoncé en version étudiante\n",
    "\n",
    "fname = \"4_Canevas_velib-corr.ipynb\" # ce fichier\n",
    "fout  = fname.replace(\"-corr\",\"\")\n",
    "\n",
    "# print(\"Fichier de sortie: \", fout )\n",
    "\n",
    "f = open(fname, \"r\")\n",
    "txt = f.read()\n",
    " \n",
    "f.close()\n",
    "\n",
    "\n",
    "f2 = open(fout, \"w\")\n",
    "f2.write(re.sub(\"<CORRECTION>.*?(</CORRECTION>)\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
